{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"PySpark","name":"pysparkkernel","language":""},"language_info":{"mimetype":"text/x-python","pygments_lexer":"python2","name":"pyspark","codemirror_mode":{"version":2,"name":"python"}},"colab":{"name":"Exercise10_DF_SparkSQL_Solution.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"T50W_m6GrLtt"},"source":["# <center>Big Data for Engineers &ndash; Exercises &ndash; Solution</center>\n","## <center>Spring 2021 &ndash; Week 10 &ndash; ETH Zurich</center>\n","## <center>Spark Dataframes and SparkSQL</center>\n","\n","## Introduction\n","For this exercise please create a Spark cluster on Azure as in the previous exercises. \n","- If you have performance problems, check the yarn UI (```https://<cluster_name>.azurehdinsight.net/yarnui/hn/cluster```) and make sure that there are no unwanted applications hogging the resources.\n","\n","## Getting the data\n","\n","\n","- Log in using ssh to your cluster:  ```ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net```\n","- Download the data: ```wget https://bigdata2020exassets.blob.core.windows.net/ex09/orders.jsonl```\n","- Upload the data to HDFS: ```hdfs dfs -put orders.jsonl /tmp/```\n","\n","\n","After you have uploaded the dataset into the Azure Blob, upload this notebook onto the Spark Jupyter server  (`https://<cluster-name>.azurehdinsight.net/jupyter`)."]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"kCQP9cPLrLt4"},"source":["## Spark Session\n","When you execute the first cell in this notebook, a spark session will be created automatically, you can print information about the session with the ```%%info``` magics"]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":229.733154296875,"end_time":1620477781437.842}},"deletable":true,"id":"Z3yDoqoyrLt5","outputId":"01561a99-fe0b-4777-ceac-efa458999b32"},"source":["print(\"Hello\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting Spark application\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table>\n","<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1620476381790_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-zirun.tchfvdineqeedhchekwtp4mjgh.dx.internal.cloudapp.net:8088/proxy/application_1620476381790_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-zirun.tchfvdineqeedhchekwtp4mjgh.dx.internal.cloudapp.net:30060/node/containerlogs/container_1620476381790_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["SparkSession available as 'spark'.\n","/usr/bin/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n","  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')Hello"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":39.2109375,"end_time":1573739034651.142}},"deletable":true,"id":"jx14Kb4prLt7"},"source":["%%info"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"BayGphrNrLt8"},"source":["By default the session is created with 3 Spark executors. We can change the configuration with the ```%%configure``` magics. Make sure you have enough vCores/Memory. You can see your total amount of available resources in yarnUI. "]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":9397.053955078125,"end_time":1573739104512.548}},"deletable":true,"id":"KUN1dhclrLt9"},"source":["%%configure -f\n","{\"executorMemory\": \"2G\", \"numExecutors\": 6, \"driverMemory\": \"4G\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"QNDasOTzrLt-"},"source":["## 1. Spark Dataframes\n","\n","Spark Dataframes allow the user to perform simple and efficient operations on data, as long as the data is structured and has a schema. Dataframes are similar to relational tables in relational databases: conceptually a dataframe is a specialization of a Spark RDD with schema information attached. You can find more information in Karau, H. et al. (2015). Learning Spark, Chapter 9 (optional reading)."]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"99J-IYGzrLt-"},"source":["### 1.1. Data preprocessing"]},{"cell_type":"code","metadata":{"scrolled":true,"cell_status":{"execute_time":{"duration":7495.68408203125,"end_time":1620481449741.919}},"deletable":true,"editable":true,"id":"evwMo2asrLt_","outputId":"e895e251-aeed-4921-b7c9-592377214aa9"},"source":["import json\n","\n","path = \"/tmp/orders.jsonl\"\n","orders_df = spark.read.json(path).cache()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting Spark application\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table>\n","<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1620476381790_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-zirun.tchfvdineqeedhchekwtp4mjgh.dx.internal.cloudapp.net:8088/proxy/application_1620476381790_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-zirun.tchfvdineqeedhchekwtp4mjgh.dx.internal.cloudapp.net:30060/node/containerlogs/container_1620476381790_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["SparkSession available as 'spark'.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"YkrETYaBrLt_"},"source":["The type of our dataset object is DataFrame"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":35.862060546875,"end_time":1573665101742.127}},"deletable":true,"editable":true,"id":"tr6Ng12YI7Du","outputId":"343b7e46-204f-464f-8325-f917001e0788"},"source":["type(orders_df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"fdB-a8xnI7Du"},"source":["Print the schema"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":251.81103515625,"end_time":1573665103317.247}},"deletable":true,"editable":true,"id":"0lV6WdLQI7Du","outputId":"6a165e9f-277a-4184-9b74-d74cc3e92d54"},"source":["orders_df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- customer: struct (nullable = true)\n"," |    |-- first_name: string (nullable = true)\n"," |    |-- last_name: string (nullable = true)\n"," |-- date: string (nullable = true)\n"," |-- items: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- price: double (nullable = true)\n"," |    |    |-- product: string (nullable = true)\n"," |    |    |-- quantity: long (nullable = true)\n"," |-- order_id: long (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"RulamP9RI7Du"},"source":["Print one row"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":3283.30908203125,"end_time":1573665107643.345}},"deletable":true,"editable":true,"id":"KJ7JYfnZI7Du","outputId":"4c78514c-6c9a-4482-9d42-8bd6797c360a"},"source":["orders_df.limit(1).collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(customer=Row(first_name='Preston', last_name='Landry'), date='2018-2-4', items=[Row(price=1.53, product='fan', quantity=5), Row(price=1.33, product='computer screen', quantity=6), Row(price=1.06, product='kettle', quantity=6), Row(price=1.96, product='stuffed animal', quantity=3), Row(price=1.09, product='the book', quantity=7), Row(price=1.42, product='headphones', quantity=9), Row(price=1.67, product='whiskey bottle', quantity=3)], order_id=0)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"ZJOQJZwlI7Du"},"source":["You can access the underlying RDD object and use any functions you learned for Spark RDDs."]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":17329.39501953125,"end_time":1573665345486.969}},"deletable":true,"editable":true,"id":"v8Nw_5rmI7Dv","outputId":"6f75187a-3803-4d39-d77d-6b0188c133a6"},"source":["orders_df.rdd.filter(lambda ordr: ordr.customer.last_name == \"Landry\").count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1960"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"4gHxD8vNI7Dv"},"source":["### 1.2. Dataframe Operations\n","We perform some queries using operations on Dataframes ([Here](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#untyped-dataset-operations-aka-dataframe-operations) is a guide on DF Operations with a link to the [API Documentation](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html))"]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"Ae6QeA9YI7Dv"},"source":["We can select columns and show the result"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":252.5771484375,"end_time":1573665989686.293}},"deletable":true,"editable":true,"id":"pxG1k0FKI7Dv","outputId":"2cd3d260-6989-429d-d965-0c5cebede217"},"source":["orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+---------+\n","|first_name|last_name|\n","+----------+---------+\n","|   Preston|   Landry|\n","|    Jamari|Dominguez|\n","|   Brendon|  Sicilia|\n","|    Armani|   Ardeni|\n","|    Jamari|     Miao|\n","+----------+---------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"zbfzPHCmI7Dv"},"source":["As you can see we can navigate to the nested items with the dot"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":2263.60888671875,"end_time":1573665774856.528}},"deletable":true,"editable":true,"id":"CY7FDKGcI7Dv","outputId":"15d45795-3af9-4497-9b83-cf820d5d37d6"},"source":["orders_df.filter(orders_df[\"customer.last_name\"] == \"Landry\").count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1960"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"W5V7_pSRI7Dw"},"source":["How about nested arrays?"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":251.12890625,"end_time":1573666229796.764}},"deletable":true,"editable":true,"id":"nL8TxoZRI7Dw","outputId":"02d87438-824f-45c9-bf55-86bf5815f605"},"source":["orders_df.select(\"order_id\", \"items\").orderBy(\"order_id\").limit(5).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+--------------------+\n","|order_id|               items|\n","+--------+--------------------+\n","|       0|[[1.53, fan, 5], ...|\n","|       1|[[1.61, fan, 7], ...|\n","|       2|[[1.41, the book,...|\n","|       3|[[1.05, computer ...|\n","|       4|[[1.92, headphone...|\n","+--------+--------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"kN7ySTnGI7Dw"},"source":["Let us try to find orders of a fan."]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":239.119140625,"end_time":1573666737735.271}},"deletable":true,"editable":true,"id":"PpX6QYOaI7Dw","outputId":"52755ede-200d-46c6-ba24-3460b5b6818c"},"source":["orders_df.filter(orders_df[\"items.product\"] == \"fan\").count()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"cannot resolve '(`items`.`product` = 'fan')' due to data type mismatch: differing types in '(`items`.`product` = 'fan')' (array<string> and string).;;\n'Filter (items#9.product = fan)\n+- Relation[customer#7,date#8,items#9,order_id#10L] json\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-26-b4bedc5c6551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morders_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morders_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"items.product\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"fan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32mc:\\users\\shuaizhang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   1459\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"condition should be string or Column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\shuaizhang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\shuaizhang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\shuaizhang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n","\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '(`items`.`product` = 'fan')' due to data type mismatch: differing types in '(`items`.`product` = 'fan')' (array<string> and string).;;\n'Filter (items#9.product = fan)\n+- Relation[customer#7,date#8,items#9,order_id#10L] json\n"]}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"MGq1NjntI7Dw"},"source":["The above code doesn't work! Use ```array contains``` instead."]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":251.64599609375,"end_time":1573666726393.938}},"deletable":true,"editable":true,"id":"jLASULZuI7Dw","outputId":"776e68ff-da8b-47f5-b45f-f79ed232ab45"},"source":["from pyspark.sql.functions import array_contains\n","\n","orders_df.filter(array_contains(\"items.product\", \"fan\")).count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32778"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"qAuJJGGRI7Dx"},"source":["Let us try to unnest the data.\n","\n","Unnest the products with explode.\n","\n","Explode will generate as many rows as there are elements in the array and match them to other attributes."]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":1255.80712890625,"end_time":1573666787807.612}},"deletable":true,"editable":true,"id":"JzGEFcZoI7Dx","outputId":"aae40b79-1adc-454a-9917-44a68d535725"},"source":["from pyspark.sql.functions import explode\n","\n","orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").orderBy(\"order_id\").limit(5).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------------+---------------+--------+\n","|                   i|        product|order_id|\n","+--------------------+---------------+--------+\n","|      [1.53, fan, 5]|            fan|       0|\n","|[1.33, computer s...|computer screen|       0|\n","|   [1.06, kettle, 6]|         kettle|       0|\n","|[1.96, stuffed an...| stuffed animal|       0|\n","| [1.09, the book, 7]|       the book|       0|\n","+--------------------+---------------+--------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"Z0zlLnp4I7Dx"},"source":["Now we can use this table to filter."]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":746.837158203125,"end_time":1573667003917.751}},"deletable":true,"editable":true,"id":"cdGtZJR4I7Dx","outputId":"e1645158-d7c2-4b6b-87b5-04a44c0a25e4"},"source":["exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\")\n","exploded_df.filter(exploded_df[\"product\"] == \"fan\").count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39922"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"V3MFtKmEI7Dx"},"source":["You might have tried to access the i.product column directly using a ```.filter``` right after the ```.select```. That, however, does not work, because the column is not available to ```orders_df``` when creating a clause like ```(orders_df[\"i.product\"] == \"fan\")```. A possible workaround when using Dataframe operations is that of using a string clause in ```.filter```, so that the product column will be resolved after it has been added with the ```.select```."]},{"cell_type":"code","metadata":{"scrolled":true,"cell_status":{"execute_time":{"duration":247.906005859375,"end_time":1573667777707.59}},"deletable":true,"editable":true,"id":"RXzDgj5AI7Dy","outputId":"55ccc9d6-a866-4ca4-c5dd-16938d4c82b3"},"source":["orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").filter(\"product = 'fan'\").count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39922"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"smURKnCRI7Dy"},"source":["Project the nested columns"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":269.365966796875,"end_time":1573669285846.051}},"deletable":true,"editable":true,"id":"KwxIIJbNI7Dy","outputId":"73d78411-443a-4bb5-8b89-c86e8a713d46"},"source":["orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n","    \"order_id\", \"customer.*\", \"date\", \"i.*\").limit(3).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+----------+---------+--------+-----+---------------+--------+\n","|order_id|first_name|last_name|    date|price|        product|quantity|\n","+--------+----------+---------+--------+-----+---------------+--------+\n","|       0|   Preston|   Landry|2018-2-4| 1.53|            fan|       5|\n","|       0|   Preston|   Landry|2018-2-4| 1.33|computer screen|       6|\n","|       0|   Preston|   Landry|2018-2-4| 1.06|         kettle|       6|\n","+--------+----------+---------+--------+-----+---------------+--------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"gxdSOAELI7Dy"},"source":["### 1.3 Exercises"]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"xbaeoySMI7Dy"},"source":["1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. (Hint: you may need to import the function ```desc``` from ```pyspark.sql.functions``` to define descending order)"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":4287.89892578125,"end_time":1573675535490.617}},"deletable":true,"editable":true,"id":"y2-nJF5pI7Dy","outputId":"cd9be0ce-1671-4f0f-9892-ea6266f1cd06"},"source":["from pyspark.sql.functions import desc\n","\n","orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n","    \"i.product\", \"i.quantity\"\n",").groupBy(\"product\").avg(\"quantity\").orderBy(desc(\"avg(quantity)\")).limit(10).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-----------------+\n","|        product|    avg(quantity)|\n","+---------------+-----------------+\n","|        toaster|5.515549016184942|\n","|       the book|5.514178678641427|\n","|         kettle|5.512053325314489|\n","|computer screen|5.504839685420448|\n","|     mouse trap|5.503895651308093|\n","|            fan|5.496342868593758|\n","|     headphones|5.485920795060985|\n","|       notebook|5.483182341458532|\n","| whiskey bottle|5.475555222463714|\n","| stuffed animal|5.470854598218753|\n","+---------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"Vy3TVhpII7Dy"},"source":["2) Find the most expensive order"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":2290.953125,"end_time":1573669705281.358}},"deletable":true,"editable":true,"id":"LrY_mSnoI7Dz","outputId":"89e7a620-03ec-4265-f02e-36f9c6174cc9"},"source":["exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n","exploded_df.select(\n","    \"order_id\", (exploded_df[\"i.quantity\"] * exploded_df[\"i.price\"]).alias(\"total\")\n",").groupBy(\"order_id\").sum(\"total\").orderBy(desc(\"sum(total)\")).limit(1).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+------------------+\n","|order_id|        sum(total)|\n","+--------+------------------+\n","|   99636|104.95999999999998|\n","+--------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"afpPBhRBrLuR"},"source":["## 2. Spark SQL\n","\n","Spark SQL allows the users to formulate their queries using SQL. The requirement is the use of Dataframes, which as said before are similar to relational tables. In addition to a familiar interface, writing queries in SQL might provide better performance than RDDs, inheriting efficiency from the Dataframe operations, while also performing automatic optimization of queries."]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"px2Upx-XrLuR"},"source":["In order to use sql we need to create a temporary table.\n","\n","This table only exists for the current session."]},{"cell_type":"code","metadata":{"scrolled":true,"cell_status":{"execute_time":{"duration":1077.370849609375,"end_time":1620481459223.37}},"deletable":true,"collapsed":true,"editable":true,"id":"6ER4rWAErLuR"},"source":["orders_df.registerTempTable(\"orders\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"gjLuAbBDrLuR"},"source":["### 2.1 Queries"]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"tRtQlLcorLuS"},"source":["Finally, run SQL queries on the registered tables. We will run the same queries as during the previous section, but with SQL.\n","\n","As you can see we can navigate to the nested items with the dot."]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":11478.6259765625,"end_time":1573665795839.541}},"deletable":true,"id":"08YF-SzQrLuS"},"source":["%%sql\n","-- Finally, run SQL queries on the registered tables\n","-- As you can see we can navigate to the nested items with the dot\n","SELECT count(*)\n","FROM orders\n","WHERE orders.customer.last_name == \"Landry\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"mLi6yF3NrLuS"},"source":["How about nested arrays?"]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":2276.55419921875,"end_time":1573666251672.414}},"deletable":true,"id":"ueKrbWtprLuT"},"source":["%%sql\n","-- How about nested arrays?\n","SELECT order_id, items\n","FROM orders AS o\n","ORDER BY order_id\n","LIMIT 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"3enkDNcNrLuT"},"source":["Let us try to find orders of a fan."]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":248.202880859375,"end_time":1573666528302.263}},"deletable":true,"id":"4J1iegPzrLuU"},"source":["%%sql \n","SELECT count(*)\n","FROM orders\n","WHERE items.product = \"fan\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"tksvS7EMrLuU"},"source":["The above code doesn't work! Use ```array contains``` instead."]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":752.942138671875,"end_time":1573666530734.473}},"deletable":true,"id":"2ciacrCvrLuU"},"source":["%%sql\n","\n","SELECT count(*)\n","FROM orders\n","WHERE array_contains(items.product, \"fan\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"oCidK2FCrLuV"},"source":["Let us try to unnest the data.\n","\n","Unnest the products with explode.\n","\n","Explode will generate as many rows as there are elements in the array and match them to other attributes."]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":772.9169921875,"end_time":1573667016192.464}},"deletable":true,"id":"hzQNPK2FrLuV"},"source":["%%sql\n","SELECT explode(items) as i, i.product, order_id\n","FROM orders\n","ORDER BY order_id\n","limit 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"pk8iNQLDrLuV"},"source":["Now we can use this table to filter."]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":3281.930908203125,"end_time":1573667022422.047}},"deletable":true,"id":"Lyv99R83rLuW"},"source":["%%sql\n","-- Filter on product\n","SELECT count(*)\n","    FROM (\n","    SELECT explode(items) as i, i.product, order_id\n","    FROM orders\n","    ORDER BY order_id\n","    )\n","WHERE product = \"fan\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"JuaBXyXGrLuW"},"source":["You might have tried to access the i.product column directly in the same ```SELECT``` clause. That, however, does not work, because the column is not available to the ```WHERE``` clause. In order to access the built columns directly, we need to unnest the data and make it part of our ```FROM``` clause. ```LATERAL VIEW``` lets us do just that, matching each non-array attribute to an unnested row from the array.  "]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":770.024169921875,"end_time":1573667932258.994}},"deletable":true,"id":"VijVTv2xrLuX"},"source":["%%sql\n","SELECT *\n","FROM orders lateral view explode(items) as flat_items\n","WHERE flat_items.product = \"fan\"\n","ORDER BY order_id\n","LIMIT 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"EsCKrVQ1rLuX"},"source":["Project the nested columns"]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":2275.98095703125,"end_time":1573667943996.512}},"deletable":true,"id":"S39WmNLBrLuX"},"source":["%%sql\n","SELECT order_id, customer.first_name, customer.last_name, date, flat_items.*\n","FROM orders lateral view explode(items) item_table as flat_items\n","WHERE flat_items.product = \"fan\"\n","ORDER BY order_id\n","LIMIT 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"z_a0SvIrrLuX"},"source":["Having built an unnested table, we can now easily aggregate over the previously nested columns"]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"Q6Ude1QurLuY"},"source":["### 2.2 Exercises"]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"XW3iP8J3rLuY"},"source":["1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. "]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":2274.546142578125,"end_time":1573669714658.905}},"deletable":true,"id":"gjpEnAhErLuY"},"source":["%%sql\n","SELECT flat_items.product, AVG(flat_items.quantity) as av_quantity\n","FROM orders lateral view explode(items) flat_table flat_items\n","GROUP BY flat_items.product\n","ORDER BY av_quantity DESC\n","LIMIT 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"O0lmVfTfrLuY"},"source":["2) Find the most expensive order"]},{"cell_type":"code","metadata":{"collapsed":true,"editable":true,"cell_status":{"execute_time":{"duration":1268.054931640625,"end_time":1573669716818.317}},"deletable":true,"id":"UDofEe7MrLuY"},"source":["%%sql\n","SELECT order_id, SUM(flat_items.quantity * flat_items.price) as total\n","FROM orders lateral view explode(items) flat_table flat_items\n","GROUP BY order_id\n","ORDER BY total desc\n","LIMIT 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"7qPak0E_rLuZ"},"source":["## 3. Create Nestedness\n","\n","We've already had a look at the solution of dataframes/SparkSQL towards unnesting arrays by using <font face=\"courier\">explode</font> method. For the other way round, Spark Dataframes / Spark SQL also provide ways for us to nest our data by creating arrays, especially after clauses like <font face=\"courier\">group by</font>."]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"MCUYQYzawGkZ"},"source":["In traditional PostgreSQL, we have to use one of the aggregation functions (<font face=\"courier\">max, sum, count,</font> ...) to process the result after the <font face=\"courier\">group by</font> operation. For example, for each customer (assume there are no customers with both the same first name and last name), we want to find out how many dates at which they placed an order. The query should look like this (fill in the queries yourself):"]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":11936.587890625,"end_time":1620477905132.344}},"deletable":true,"id":"5FI98iZ70Ipm","outputId":"70643201-9a3c-4f55-f782-58ef447d339e"},"source":["from pyspark.sql.functions import countDistinct\n","orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(countDistinct(\"date\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+----------+--------------------+\n","|first_name| last_name|count(DISTINCT date)|\n","+----------+----------+--------------------+\n","|      Zane|      Dahl|                   3|\n","|    Dorian|      Dahl|                   2|\n","|     Annie|     Dower|                   2|\n","|    Landen| Galatioto|                   2|\n","|     Allan|        Po|                   4|\n","|  Clarissa|   Sicilia|                   2|\n","|     Ariel|   Coulson|                   3|\n","|   Xiomara| Christofi|                   2|\n","|     Rylie|      Dahl|                   1|\n","|    Azaria|Berenguier|                   2|\n","|     Allie|Berenguier|                   4|\n","|    Daniel|   Schuler|                   1|\n","|     Chana|   Balster|                   2|\n","|     Steve|    Badash|                   3|\n","|     Jaida|    Corfas|                   2|\n","|   Gabriel|   Suchoff|                   3|\n","|   Presley|   Coulson|                   2|\n","|   Janelle|     Mayer|                   1|\n","|   Dashawn|  Gottardo|                   2|\n","|     Terry| Ho-stuart|                   2|\n","+----------+----------+--------------------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":14957.180908203125,"end_time":1620477984175.839}},"deletable":true,"id":"FunDHOxO1zPs","colab":{"referenced_widgets":["fd7fe3bdf4c64325a8320ff980c20670","a9efa0cac1714601a48834f0af4eb3fb"]},"outputId":"8835267b-ab38-4bc5-ce5f-32ca505001bd"},"source":["%%sql\n","select customer.first_name, customer.last_name, count(distinct date) from orders group by customer.first_name, customer.last_name"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd7fe3bdf4c64325a8320ff980c20670"}},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9efa0cac1714601a48834f0af4eb3fb"}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"dsTLU8ml2ORG"},"source":["But what if we are interested not only in the number of dates, but the actual\n","dates themselves? Luckily Spark Dataframes / Spark SQL do provide us with methods to preserve the original information of the date list. If now we would like to know for each customer, on which dates they placed an order, we shall use <font face=\"courier\">collect_set</font> method:"]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":1418.15087890625,"end_time":1620478080466.68}},"deletable":true,"id":"h1oGXmWt3gss","outputId":"b6c68a25-93a3-4f82-e10a-845b85479b74"},"source":["from pyspark.sql.functions import collect_set\n","orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(collect_set(\"date\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+--------------------+--------------------+\n","|first_name|           last_name|   collect_set(date)|\n","+----------+--------------------+--------------------+\n","|     Abbie|                Egan|[2018-4-8, 2016-3...|\n","|  Abigayle|           Mc namara|[2016-4-2, 2017-6-9]|\n","|   Adalynn|              Ardeni|[2018-2-2, 2018-6...|\n","|      Aden|          Rosenbloom|         [2016-3-10]|\n","|    Adonis|              Badash|          [2017-6-8]|\n","|   Agustin|          Srivastava|         [2018-4-10]|\n","|     Aiden|             Suchoff|          [2018-2-8]|\n","|    Aiyana|              Landry|          [2018-2-3]|\n","|    Alaina|              Gruber|[2016-3-1, 2018-5-1]|\n","|    Alayna|               Mayer|         [2016-3-10]|\n","|Alexandria|         Butterfield|[2018-5-3, 2018-3...|\n","|Alexzander|              Landry|[2017-1-3, 2017-6-3]|\n","|     Alice|             Balster|[2018-4-4, 2017-4-7]|\n","|     Allan|                  Po|[2016-1-9, 2016-5...|\n","|     Allie|          Berenguier|[2016-2-1, 2018-4...|\n","|    Alyvia|              Ardeni|          [2017-4-8]|\n","|     Amari|           Bridgeman|[2017-3-8, 2018-1-7]|\n","|     Amiah|Fernandez cifuentes |[2016-1-8, 2017-1...|\n","|  Anderson|              Zapata|          [2016-5-7]|\n","|    Andres|                  Mo|          [2017-6-2]|\n","+----------+--------------------+--------------------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":10117.97900390625,"end_time":1620478640139.884}},"deletable":true,"id":"aJW1JpzQ3lEV","colab":{"referenced_widgets":["5eca7ac4210c44e6b45b37e7cac4bf34","439cfb14c8d64843871c09a1c0be3d60"]},"outputId":"68d8c40e-ba42-4245-94a5-ab49ae60da27"},"source":["%%sql\n","select customer.first_name, customer.last_name, collect_set(\"date\") from orders group by customer.first_name, customer.last_name"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5eca7ac4210c44e6b45b37e7cac4bf34"}},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"439cfb14c8d64843871c09a1c0be3d60"}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"editable":true,"deletable":true,"id":"t2rTR-O78gy3"},"source":["For some other cases where we want to preserve all the entries rather than the de-duplicated ones, we can use  <font face=\"courier\">collect_list</font> method. For example, for each date we want to record the first and last names of customers. Since two customers might share the same last name, we need to collect all of them. The query should look like this:"]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":12271.528076171875,"end_time":1620480910312.875}},"deletable":true,"id":"X_PB-uAH9I5M","outputId":"f76c10ee-f085-4121-87af-0a0846061efb"},"source":["from pyspark.sql.functions import collect_list\n","orders_df.groupBy(\"date\").agg(collect_list(\"customer.last_name\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+--------------------------------+\n","|     date|collect_list(customer.last_name)|\n","+---------+--------------------------------+\n","| 2017-4-8|            [Miao, Decaro, Da...|\n","| 2016-2-5|            [Drago, Mignani, ...|\n","| 2016-1-1|            [Suchoff, Lowe, D...|\n","| 2018-2-6|            [Gottardo, Po, Go...|\n","| 2018-1-7|            [Galatioto, Migna...|\n","| 2016-6-7|            [Ardeni, Kerman, ...|\n","|2018-5-10|            [Ho-stuart, Galat...|\n","| 2017-5-4|            [Srivastava, Ho-s...|\n","| 2018-4-7|            [Drago, Mayer, La...|\n","|2018-6-10|            [Lowe, Dominguez,...|\n","| 2017-3-3|            [Dahlstedt, Lemay...|\n","| 2017-3-5|            [Balster, Horah, ...|\n","| 2017-6-4|            [Findley, Marinko...|\n","| 2018-6-3|            [Cerda, Gruber, S...|\n","| 2018-1-6|            [Zapata, Miao, Ne...|\n","| 2016-4-9|            [Badash, Dahlsted...|\n","| 2016-1-5|            [Gottardo, Ho-stu...|\n","|2018-1-10|            [Landry, Badash, ...|\n","| 2017-1-5|            [Dower, Zapata, M...|\n","| 2018-1-4|            [Egan, Schuler, B...|\n","+---------+--------------------------------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"code","metadata":{"editable":true,"cell_status":{"execute_time":{"duration":30449.055908203125,"end_time":1620481513099.958}},"deletable":true,"id":"z7DSOsKC9dmF","colab":{"referenced_widgets":["a2f4e8e80a2e424ba0e552e89b16a8a7","2b9d668dbcfa4a639a140d117b5e3902"]},"outputId":"eae40ebd-63f9-4ec1-9742-f1bb391d1071"},"source":["%%sql\n","select \"date\", collect_list(customer.last_name) from orders group by \"date\""],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2f4e8e80a2e424ba0e552e89b16a8a7"}},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b9d668dbcfa4a639a140d117b5e3902"}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"KoJbUmwrQOmy"},"source":["Now try it on yourself.\n","\n","For every order in 2016-1-1, return the list of products that appeared in that order:"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":90.6279296875,"end_time":1620480224562.293}},"id":"GpWf6W_ZPg9-","outputId":"59ca0ced-a3e0-46a2-a010-445e5efc372a"},"source":["from pyspark.sql.functions import explode\n","exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n","exploded_df.filter(exploded_df[\"date\"] == \"2016-1-1\").groupBy(\"order_id\").agg(collect_list(\"i.product\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+-----------------------+\n","|order_id|collect_list(i.product)|\n","+--------+-----------------------+\n","|    3120|   [whiskey bottle, ...|\n","+--------+-----------------------+"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ifIhGcEQmrT"},"source":["For every product, return the set of dates at which it's purchased:"]},{"cell_type":"code","metadata":{"cell_status":{"execute_time":{"duration":996.149169921875,"end_time":1620480623060.309}},"id":"ZEmGI5GUPg9-","outputId":"dafebd33-ad91-4566-95cf-2942cbd25ba3"},"source":["from pyspark.sql.functions import collect_set\n","exploded_df.orderBy(\"date\").groupBy(\"i.product\").agg(collect_set(\"date\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+--------------------+\n","|        product|   collect_set(date)|\n","+---------------+--------------------+\n","|       the book|[2018-2-1, 2018-4...|\n","|     mouse trap|[2018-3-9, 2018-4...|\n","|computer screen|[2018-2-1, 2018-4...|\n","| whiskey bottle|[2018-3-9, 2018-4...|\n","|        toaster|[2018-2-1, 2018-4...|\n","| stuffed animal|[2018-2-1, 2018-4...|\n","|         kettle|[2018-3-9, 2018-4...|\n","|            fan|[2018-2-1, 2018-4...|\n","|     headphones|[2018-3-9, 2018-4...|\n","|       notebook|[2018-2-1, 2018-4...|\n","+---------------+--------------------+"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uboC94LIRThc"},"source":["One of the drawbacks of the <font face=\"courier\">collect_set/collect_list</font> method is they only accept one column as the argument. Later we will see how we can create nestedness on pretty much everything after we get the hang of the mighty JSONiq."]}]}