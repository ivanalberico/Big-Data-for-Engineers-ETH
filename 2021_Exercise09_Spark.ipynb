{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers – Exercises</center>\n",
    "## <center>Spring 2021 – Week 9 – ETH Zurich</center>\n",
    "## <center>Spark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the Spark cluster on Azure\n",
    "\n",
    "### Create a cluster\n",
    "\n",
    "1. Sign into the azure portal (portal.azure.com).\n",
    "1. Search for \"HDInsight clusters\" using the search box at the top.\n",
    "1. Create a new resource group (for example: 'exercise09').\n",
    "1. Give the cluster a unique name (for example: 'exercise09cluster').\n",
    "1. In the \"Cluster Type\" choose **Spark** and leave the default version as is. It is also indicated to use the **UK South** region. \n",
    "1. Create a cluster login password (you can use https://www.random.org/strings/ for inspiration). Keep the password around as you will need it for later!\n",
    "1. Move to the *Storage* stage of the setup. Here, leave **Azure Storage** as the *Primary Storage Type*. For the *Primary Storage Account* you have the option to set up a new account. The *Container*'s name will be generated automatically, however make sure to remember it, or change it to something memorable, if you plan on finishing the exercises in more than one sitting.\n",
    "1. Move to the *Configuration + Pricing* stage of the setup (skip *Security + networking*). Set up a Spark cluster which uses **D12 v2** deployments for both the *Head* and *Worker* nodes. Additionally, you should select to deploy *4* *Worker* nodes. It should cost roughly 2.2 CHF/h. Notice that you are running a relatively sizable cluster: $24 \\text{ cores} = 4 \\text{ cores} * 6 \\text{ nodes}$, and $168 \\text{GB} = 28 \\text{GB} * 6 \\text{ nodes}$. \n",
    "1. Move to the *Reivew + Create* stage of the setup, and click the **Create** button once validation succeeds.\n",
    "1. Wait until your cluster is deployed (this can take up to 20 minutes).\n",
    "\n",
    "## <span style=\"color: red;\">**Important:** Remember to **delete** the cluster once you are done. If you want to stop doing the exercises at any point, delete it and recreate it using the same container name as you used the first time, so that the resources are still there.</span>\n",
    "<img src=\"https://bigdataforengineers2021.blob.core.windows.net/exercise09/tutorial_delete.png\" style=\"width: 900px;\">\n",
    "\n",
    "### Access your cluster\n",
    "\n",
    "Make sure you can access your cluster (the NameNode) via SSH:\n",
    "\n",
    "```\n",
    "$ ssh ssh_user_name@cluster_name-ssh.azurehdinsight.net\n",
    "```\n",
    "\n",
    "If you are using Linux or MacOSX, you can use your standard terminal.\n",
    "If you are using Windows you can use:\n",
    "- Putty SSH Client and PSCP tool (get them at [here](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)).\n",
    "- This Notebook server terminal (Click on the Jupyter logo and the goto New -&gt; Terminal).\n",
    "- Azure Cloud Terminal (see the HBase exercise sheet for details)\n",
    "\n",
    "\n",
    "The cluster has its own Jupyter server. We will use it. You can access it through the following link:\n",
    "```\n",
    " https://cluster_name.azurehdinsight.net/jupyter\n",
    "```\n",
    "\n",
    "You can access cluster's YARN in your browser\n",
    "```\n",
    " https://cluster_name.azurehdinsight.net/yarnui/hn/cluster\n",
    "```\n",
    "\n",
    "The Spark UI can be accessed via Azure Portal, see [Spark job debugging](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-job-debugging)\n",
    "\n",
    "## <span >You need to upload this notebook to your cluster's Jupyter in order to execute Python code blocks.</span>\n",
    "\n",
    "\n",
    "To do this, just open the Jupyter through the link given above and use the \"Upload\" button. After the notebook has been loaded, you can open it using a Spark kernel.\n",
    "<img src=\"https://bigdataforengineers2021.blob.core.windows.net/exercise09/tutorial_jupyter.png\" style=\"width: 900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apache Spark Architecuture\n",
    "\n",
    "Spark is a cluster computing platform designed to be fast and general purpose. Spark extends the MapReduce model to efficiently cover a wide range of workloads that previously required separate distributed systems, including interactive queries and stream processing. Spark offers the ability to run computations in memory.\n",
    "\n",
    "At a high level, every Spark application consists of a **driver program** that launches various parallel operations on a cluster. The driver program contains your application's main function and defines distributed datasets on the cluster, then applies operations to them.\n",
    "\n",
    "Driver programs access Spark through a **SparkContext** object, which represents a connection to a computing cluster. There is no need to create a SparkContext; it is created for you automatically when you run the first code cell in the Jupyter\n",
    "\n",
    "The driver communicates with a potentially large number of distributed workers called **executors**. The driver runs in its own process and each executor is a separate process. A driver and its executors are together termed a Spark application.\n",
    "\n",
    "![Image of Account](http://spark.apache.org/docs/latest/img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understand resilient distributed datasets (RDD)\n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "##### What are RDD operations?\n",
    "RDDs offer two types of operations: **transformations** and **actions**.\n",
    "\n",
    "* **Transformations** create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n",
    "* **Actions** compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a **lazy** fashion, that is, the first time they are used in an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at `wasb:///example/data/fruits.txt`. Notice that we're reading from Azure Blob storage, this is indicated by the `wasb` token in the previous url. Generally it is possible to read data from other resources using the following tokens:\n",
    "\n",
    "* `file`: Read from the local file system.\n",
    "* `hdfs`: Read from a Hadoop Distributed File System.\n",
    "* `s3`  : Read from AWS S3 Storage.\n",
    "* `wasb`: Read from Azure Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T17:48:55.280470Z",
     "start_time": "2021-04-30T17:48:54.387363Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2284.843017578125,
      "end_time": 1619992986444.119
     }
    }
   },
   "outputs": [],
   "source": [
    "# sc is the Spark Context object automatically created for you\n",
    "fruits = sc.textFile('wasb:///example/data/fruits.txt')\n",
    "yellowThings = sc.textFile('wasb:///example/data/yellowthings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDD transformations\n",
    "Following are examples of some of the common transformations available. For a detailed list, see [RDD Transformations](https://spark.apache.org/docs/2.0.0/programming-guide.html#transformations)\n",
    "\n",
    "Run some transformations below to understand this better.\n",
    "\n",
    "**Note:** If some of the queries are taking too long to complete, try restarting the kernel, and rerunning the cell *above*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T16:36:47.273406Z",
     "start_time": "2021-04-30T16:36:46.938589Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 13372.175048828125,
      "end_time": 1619992999827.243
     }
    }
   },
   "outputs": [],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "fruitsReversed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T16:35:24.992136Z",
     "start_time": "2021-04-30T16:35:24.925647Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 5291.009033203125,
      "end_time": 1619993009753.202
     }
    }
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 755.614013671875,
      "end_time": 1619993010926.142
     }
    }
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 762.670166015625,
      "end_time": 1619993014486.587
     }
    }
   },
   "outputs": [],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 803.803955078125,
      "end_time": 1619993016289.369
     }
    }
   },
   "outputs": [],
   "source": [
    "# intersection\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 758.760986328125,
      "end_time": 1619993023087.109
     }
    }
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 760.218017578125,
      "end_time": 1619993044556.984
     }
    }
   },
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "for letter, lst in yellowThingsByFirstLetter.collect():\n",
    "        print(\"For letter\", letter)\n",
    "        for obj in lst:\n",
    "            print(\" > \", obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 757.301025390625,
      "end_time": 1619993047373.34
     }
    }
   },
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "# Note: the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \n",
    "numFruitsByLength.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.3.0/programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 250.887939453125,
      "end_time": 1619993067760.489
     }
    }
   },
   "outputs": [],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)\n",
    "print(yellowThingsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.635986328125,
      "end_time": 1619993079227.007
     }
    }
   },
   "outputs": [],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "numFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.071044921875,
      "end_time": 1619993081636.856
     }
    }
   },
   "outputs": [],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "first3Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.026123046875,
      "end_time": 1619993082497.212
     }
    }
   },
   "outputs": [],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lazy evaluation\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling `map()`), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as\n",
    "consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call `sc.textFile()`, the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can\n",
    "occur multiple times.\n",
    "\n",
    "\n",
    "Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. For instance, the code bellow corresponds to the following graph:\n",
    "\n",
    "<img src=\"https://bigdataforengineers2021.blob.core.windows.net/exercise09/stages.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.416015625,
      "end_time": 1619993087737.5
     }
    }
   },
   "outputs": [],
   "source": [
    "apples = fruits.filter(lambda x: \"apple\" in x)\n",
    "lemons = yellowThings.filter(lambda x: \"lemon\" in x)\n",
    "applesAndLemons = apples.union(lemons)\n",
    "print(applesAndLemons.collect())\n",
    "print(applesAndLemons.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persistence (Caching)\n",
    "\n",
    "Spark's RDDs are by default recomputed each time you run an action on\n",
    "them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`. After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions. Persisting RDDs on disk instead of memory is also possible.\n",
    "\n",
    "If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy. For the memory-only storage levels, it will recompute these partitions the next time they are accessed,\n",
    "while for the memory-and-disk ones, it will write them out to disk. In either case, this means that you don't have to worry about your job breaking if you ask Spark to cache too much data. However, caching unnecessary data can lead to eviction of useful data\n",
    "and more recomputation time. Finally, RDDs come with a method called `unpersist()` that lets you manually remove them from the cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with Key/Value Pairs\n",
    "\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. These RDDs\n",
    "are called *pair RDDs*. Pair RDDs are a useful building block in many programs, as\n",
    "they expose operations that allow you to act on each key in parallel or regroup data\n",
    "across the network. For example, pair RDDs have a `reduceByKey()` method that can\n",
    "aggregate data separately for each key, and a `join()` method that can merge two\n",
    "RDDs together by grouping elements with the same key. Pair RDDs are also still RDDs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 759.595947265625,
      "end_time": 1619993113263.276
     }
    }
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "rdd = sc.parallelize([(\"key1\", 0) ,(\"key2\", 3),(\"key1\", 8) ,(\"key3\", 3),(\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(rdd2.collect())\n",
    "print(rdd2.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting a user program into tasks\n",
    "\n",
    "A Spark driver is responsible for converting a user program into units of physical execution called tasks. At a high level, all Spark programs follow the same structure: they create RDDs from some input, derive new RDDs from those using transformations, and perform actions to collect or save data. A Spark program implicitly creates a logical **directed acyclic graph (DAG)** of operations.\n",
    "When the driver runs, it converts this logical graph into a physical execution plan.\n",
    "\n",
    "Spark performs several optimizations, such as \"pipelining\" map transformations together to merge them, and converts the execution graph into a set of **stages**.\n",
    "Each stage, in turn, consists of multiple tasks. The tasks are bundled up and prepared to be sent to the cluster. Tasks are the smallest unit of work in Spark; a typical user program can launch hundreds or thousands of individual tasks.\n",
    "\n",
    "Each RDD maintains a pointer to one or more parents along with metadata about what\n",
    "type of relationship they have. For instance, when you call `val b = a.map()` on an\n",
    "RDD, the RDD `b` keeps a reference to its parent `a`. These pointers allow an RDD to be\n",
    "traced to all of its ancestors.\n",
    "\n",
    "The following phases occur during Spark execution:\n",
    "* User code defines a DAG (directed acyclic graph) of RDDs. Operations on RDDs create new RDDs that refer back to their parents, thereby creating a graph.\n",
    "* Actions force translation of the DAG to an execution plan. When you call an action on an RDD, it must be computed. This requires computing its parent RDDs as well. \n",
    "* Spark's scheduler submits a job to compute all needed RDDs. That job will have one or more stages, which are parallel waves of computation composed of tasks. Each stage will correspond to one or more RDDs in the DAG. A single stage can correspond to multiple RDDs due to pipelining.\n",
    "* Tasks are scheduled and executed on a cluster\n",
    "* Stages are processed in order, with individual tasks launching to compute segments of the RDD. Once the final stage is finished in a job, the action is complete.\n",
    "\n",
    "If you visit the application's web UI, you will see how many stages occur in order to\n",
    "fulfill an action. The Spark UI can be accessed via Azure Portal, see [Spark job debugging](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-job-debugging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 44.55712890625,
      "end_time": 1557044256642.663
     }
    },
    "collapsed": true
   },
   "source": [
    "### 3. The Great Language Game\n",
    "\n",
    "Now, you will get to write some queries yourself on a larger dataset. You will be using the [language confusion dataset](http://lars.yencken.org/datasets/languagegame/).\n",
    "\n",
    "This exercise is a little bit different, in that it is part of a small project you will be doing over the following 3 weeks to compare Spark, Spark with DataFrames/SQL, and Sparksoniq. You will hear more about it in the coming weeks.\n",
    "\n",
    "Apart from that, you will have to submit the results of this exercise to Moodle to obtain the weekly bonus. You will need four things:\n",
    "- The query you wrote\n",
    "- Something related to its output (which you will be graded on)\n",
    "- The time it took you to write it\n",
    "- The time it took you to run it\n",
    "\n",
    "As you might have observed in the sample queries above, the time a job took to run is displayed on the rightmost column of its ouptut. If it consists of several stages, however, you will need the sum of them. The easiest thing is if you just take the execution time of the whole query:\n",
    "\n",
    "<img src=\"https://bigdataforengineers2021.blob.core.windows.net/exercise09/tutorial_time.png\" style=\"width: 700px;\">\n",
    "\n",
    "Of course, you will not be evaluated on the time it took you to write the queries (nor on the time it took them to run), but this is useful to us in order to measure the increase in performance when using Sparksoniq. There is a cell that outputs the time you started working before every query. Use this if you find it useful.\n",
    "\n",
    "\n",
    "For this exercise, you can chose to either set up a local Spark installation on your computer or use an Azure cluster as explained above. Both are fine, but a local installation really makes things easier to debug.\n",
    "\n",
    "**If you choose to go for a local installation: **\n",
    "\n",
    "- Make sure you have [Java 8](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) or later installed on your computer.\n",
    "- Install the latest [Spark](https://spark.apache.org/downloads.html) release. Here are tutorials for [Linux](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f), [Windows](https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f), and [MacOS](https://www.tutorialkart.com/apache-spark/how-to-install-spark-on-mac-os/).\n",
    "- Launch pyspark with `$ pyspark`\n",
    "\n",
    "**In any case:**\n",
    "\n",
    "Now either log in to your cluster using SSH as explained above and run the following commands, or of course do it locally:\n",
    "\n",
    "```\n",
    "wget http://data.greatlanguagegame.com.s3.amazonaws.com/confusion-2014-03-02.tbz2\n",
    "tar -jxvf confusion-2014-03-02.tbz2 -C /tmp\n",
    "```\n",
    "If you're on a cluster:\n",
    "```\n",
    "hdfs dfs -copyFromLocal /tmp/confusion-2014-03-02/confusion-2014-03-02.json /confusion.json\n",
    "```\n",
    "\n",
    "This dowloads the archive file to the cluster, decompresses it and uploads it to HDFS when using a cluster. Now, create an RDD from the file containing the entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:07:34.367427Z",
     "start_time": "2021-05-03T17:07:33.474055Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 747.39990234375,
      "end_time": 1619999839916.484
     }
    }
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('wasb:///confusion.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or locally:\n",
    "```\n",
    "data = sc.textFile('file:///tmp/confusion-2014-03-02/confusion-2014-03-02.json')\n",
    "```\n",
    "**Note:** from here on, if you're doing things locally, just copy the contents of the cells into the pyspark shell and work on there!\n",
    "\n",
    "Since the entries are JSON records, you will need to parse them and use their respective object representations. You can use this mapping for all queries. Since some of the queries take a long time to execute on the dataset, you might want to answer these queries on the first `100000` entries. Later you can run the same queries on the entire dataset, by runing queries on `entries` instead of `test_entries`. For the quiz running the queries on either dataset (100000 entries or all entries) will be accepted as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:11:24.581153Z",
     "start_time": "2021-05-03T17:11:23.900172Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 5296.226806640625,
      "end_time": 1619999845222.09
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "testset = sc.parallelize(data.take(100000))\n",
    "test_entries = testset.map(json.loads)\n",
    "\n",
    "entries = data.map(json.loads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it. Is it working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:31:32.131861Z",
     "start_time": "2021-05-03T15:31:31.959140Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 234.972900390625,
      "end_time": 1619999845467.318
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_german = test_entries.filter(lambda e: e[\"target\"] == \"German\").take(1)\n",
    "print(json.dumps(target_german, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's get to work. A few last things:\n",
    "- Take into account that some of the queries might have very large outputs, which Jupyter (or sometimes even Spark) won't be able to handle. It is normal for the queries to take some time, but if the notebook crashes or stops responding, try restarting the kernel. Avoid printing large outputs. You can print the first few entries to confirm the query has worked, as shown in query 1.\n",
    "- Remember to delete the cluster if you want to stop working! You can recreate it using the same container name and your resources will still be there.\n",
    "- Refer to the [documentation](http://spark.apache.org/docs/2.3.0/api/python/pyspark.html#pyspark.RDD), as well as the programming guides on actions and transformations linked to above.\n",
    "\n",
    "And now to the actual queries: *Please make sure that in your queries you *only* use PySpark, and avoid any dataframes (they will covered in next week's exercises)*\n",
    "\n",
    "1\\. Find all games such that the guessed language is correct (=target), and such that this language is Russian. What is the length of the resulting sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:31:38.639942Z",
     "start_time": "2021-05-03T15:31:38.632923Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 32.283935546875,
      "end_time": 1619999851034.322
     }
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:31:40.763220Z",
     "start_time": "2021-05-03T15:31:40.039089Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2256.77880859375,
      "end_time": 1619999854080.019
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. For those cases where the *guess* matched the *target*, list what the *guessed* language was. How many times has *Hindi* been correctly guessed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:33:36.332159Z",
     "start_time": "2021-05-03T15:33:36.314186Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 30.8681640625,
      "end_time": 1619999870642.782
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:33:38.006356Z",
     "start_time": "2021-05-03T15:33:36.964006Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 1255.987060546875,
      "end_time": 1619999873086.992
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Find the number of all distinct values of the *target* languages (i.e. the *target* field). What is the length of the resulting sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 29.1708984375,
      "end_time": 1619999876827.566
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:34:36.715804Z",
     "start_time": "2021-05-03T15:34:35.945578Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 1249.218017578125,
      "end_time": 1619999878828.007
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Return the top three games where the guessed language is correct (=target) ordered by language (ascending), then country (ascending), then date (ascending). What is the date of the 3rd item in the list? Enter it without quotes, for example 2013-10-02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:35:05.242570Z",
     "start_time": "2021-05-03T15:35:05.232564Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 32.781005859375,
      "end_time": 1619999881283.878
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:35:23.809109Z",
     "start_time": "2021-05-03T15:35:21.827970Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2255.52197265625,
      "end_time": 1619999884156.929
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Aggregate all games by country and target language, counting the number of guessing games that were done for each pair (country, target). How many guesses have been made for Maltese from the Netherlands (NL, Maltese)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:38:36.420721Z",
     "start_time": "2021-05-03T15:38:36.413591Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 29.47998046875,
      "end_time": 1619999890479.283
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:40:43.495660Z",
     "start_time": "2021-05-03T15:40:42.877495Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 744.037109375,
      "end_time": 1619999964374.737
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Among all the games where the guess was correct (=target), what is the percentage of cases where the first choice (among the array of possible answers) was the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:44:36.950966Z",
     "start_time": "2021-05-03T15:44:36.944227Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 31.989013671875,
      "end_time": 1619999968532.288
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:44:38.257988Z",
     "start_time": "2021-05-03T15:44:37.238548Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 1265.596923828125,
      "end_time": 1620000286240.596
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. For each target language, compute the percentage of successful guess games (i.e. *guess* == *target*) relative to all games for that target language, and display the pairs `(target_language, percentage)` in ascending order of the percentage. What is the second language in this list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:45:27.948317Z",
     "start_time": "2021-05-03T15:45:27.940925Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 40.760986328125,
      "end_time": 1620000044895.066
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:46:03.514495Z",
     "start_time": "2021-05-03T15:46:02.433609Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 1261.2919921875,
      "end_time": 1620000145674.288
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Group the games by the index of the correct answer in the choices array and output all counts. How many games the last choice is the correct choice (target)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:08:04.903363Z",
     "start_time": "2021-05-03T15:08:04.597735Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 54.723876953125,
      "end_time": 1619999082218.56
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:47:14.438875Z",
     "start_time": "2021-05-03T15:47:13.544643Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 229.343994140625,
      "end_time": 1620004678036.786
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. For the cases where both *guess* and *target* were `'French'`, what is the count of each possible number of choices (namely if you have two games with 5 choices report `(5,2)`). what is the most frequent choice length among these list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 257.208984375,
      "end_time": 1620001689823.598
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T16:27:52.070186Z",
     "start_time": "2021-05-03T16:26:40.172838Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 33358.02587890625,
      "end_time": 1620004730847.938
     }
    }
   },
   "outputs": [],
   "source": [
    "# Query:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. How many games were played on the last day? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:52:14.339678Z",
     "start_time": "2021-05-03T15:52:14.328215Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 40.007080078125,
      "end_time": 1619996219614.065
     }
    }
   },
   "outputs": [],
   "source": [
    "# Started working:\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:23:10.632593Z",
     "start_time": "2021-05-03T17:23:08.538836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exercise\n",
    "\n",
    "1. Why is Spark faster than Hadoop MapReduce?\n",
    "1. Study the queries you wrote using Spark UI. Observe how many stages they have.\n",
    "1. Which of the graphs below are DAGs?\n",
    "<img src=\"https://bigdataforengineers2021.blob.core.windows.net/exercise09/dags.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. True or False\n",
    "Say if the following statements are *true* or *false*, and explain why.\n",
    "\n",
    "1. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\n",
    "1. Transformations construct a new RDD from a previous one and immediately calculate the result\n",
    "1. Spark's RDDs are by default recomputed each time you run an action on them\n",
    "1. After computing an RDD, Spark will store its contents in memory and reuse them in future actions.\n",
    "1. When you derive new RDDs using transformations, Spark keeps track of the set of dependencies between different RDDs."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
