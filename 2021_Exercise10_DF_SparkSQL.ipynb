{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T50W_m6GrLtt"
   },
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises</center>\n",
    "## <center>Spring 2021 &ndash; Week 10 &ndash; ETH Zurich</center>\n",
    "## <center>Spark Dataframes and SparkSQL</center>\n",
    "\n",
    "## Introduction\n",
    "For this exercise please create a Spark cluster on Azure as in the previous exercises. \n",
    "- If you have performance problems, check the yarn UI (```https://<cluster_name>.azurehdinsight.net/yarnui/hn/cluster```) and make sure that there are no unwanted applications hogging the resources.\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "\n",
    "- Log in using ssh to your cluster:  ```ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net```\n",
    "- Download the data: ```wget https://bigdata2020exassets.blob.core.windows.net/ex09/orders.jsonl```\n",
    "- Upload the data to HDFS: ```hdfs dfs -put orders.jsonl /tmp/```\n",
    "\n",
    "\n",
    "After you have uploaded the dataset into the Azure Blob, upload this notebook onto the Spark Jupyter server  (`https://<cluster-name>.azurehdinsight.net/jupyter`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCQP9cPLrLt4"
   },
   "source": [
    "## Spark Session\n",
    "When you execute the first cell in this notebook, a spark session will be created automatically, you can print information about the session with the ```%%info``` magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 229.733154296875,
      "end_time": 1620477781437.842
     }
    },
    "id": "Z3yDoqoyrLt5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 39.2109375,
      "end_time": 1573739034651.142
     }
    },
    "id": "jx14Kb4prLt7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%info` not found.\n"
     ]
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BayGphrNrLt8"
   },
   "source": [
    "By default the session is created with 3 Spark executors. We can change the configuration with the ```%%configure``` magics. Make sure you have enough vCores/Memory. You can see your total amount of available resources in yarnUI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9397.053955078125,
      "end_time": 1573739104512.548
     }
    },
    "id": "KUN1dhclrLt9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%configure` not found.\n"
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\": \"2G\", \"numExecutors\": 6, \"driverMemory\": \"4G\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNDasOTzrLt-"
   },
   "source": [
    "## 1. Spark Dataframes\n",
    "\n",
    "Spark Dataframes allow the user to perform simple and efficient operations on data, as long as the data is structured and has a schema. Dataframes are similar to relational tables in relational databases: conceptually a dataframe is a specialization of a Spark RDD with schema information attached. You can find more information in Karau, H. et al. (2015). Learning Spark, Chapter 9 (optional reading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99J-IYGzrLt-"
   },
   "source": [
    "### 1.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 7495.68408203125,
      "end_time": 1620481449741.919
     }
    },
    "id": "evwMo2asrLt_",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-58b116629fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/tmp/orders.jsonl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0morders_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"/tmp/orders.jsonl\"\n",
    "orders_df = spark.read.json(path).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkrETYaBrLt_"
   },
   "source": [
    "The type of our dataset object is DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35.862060546875,
      "end_time": 1573665101742.127
     }
    },
    "id": "tr6Ng12YI7Du"
   },
   "outputs": [],
   "source": [
    "type(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdB-a8xnI7Du"
   },
   "source": [
    "Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.81103515625,
      "end_time": 1573665103317.247
     }
    },
    "id": "0lV6WdLQI7Du"
   },
   "outputs": [],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RulamP9RI7Du"
   },
   "source": [
    "Print one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3283.30908203125,
      "end_time": 1573665107643.345
     }
    },
    "id": "KJ7JYfnZI7Du"
   },
   "outputs": [],
   "source": [
    "orders_df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJOQJZwlI7Du"
   },
   "source": [
    "You can access the underlying RDD object and use any functions you learned for Spark RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17329.39501953125,
      "end_time": 1573665345486.969
     }
    },
    "id": "v8Nw_5rmI7Dv"
   },
   "outputs": [],
   "source": [
    "orders_df.rdd.filter(lambda ordr: ordr.customer.last_name == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gHxD8vNI7Dv"
   },
   "source": [
    "### 1.2. Dataframe Operations\n",
    "We perform some queries using operations on Dataframes ([Here](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#untyped-dataset-operations-aka-dataframe-operations) is a guide on DF Operations with a link to the [API Documentation](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae6QeA9YI7Dv"
   },
   "source": [
    "We can select columns and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 252.5771484375,
      "end_time": 1573665989686.293
     }
    },
    "id": "pxG1k0FKI7Dv"
   },
   "outputs": [],
   "source": [
    "orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbfzPHCmI7Dv"
   },
   "source": [
    "As you can see we can navigate to the nested items with the dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2263.60888671875,
      "end_time": 1573665774856.528
     }
    },
    "id": "CY7FDKGcI7Dv"
   },
   "outputs": [],
   "source": [
    "orders_df.filter(orders_df[\"customer.last_name\"] == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5V7_pSRI7Dw"
   },
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.12890625,
      "end_time": 1573666229796.764
     }
    },
    "id": "nL8TxoZRI7Dw"
   },
   "outputs": [],
   "source": [
    "orders_df.select(\"order_id\", \"items\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN7ySTnGI7Dw"
   },
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 239.119140625,
      "end_time": 1573666737735.271
     }
    },
    "id": "PpX6QYOaI7Dw"
   },
   "outputs": [],
   "source": [
    "orders_df.filter(orders_df[\"items.product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGq1NjntI7Dw"
   },
   "source": [
    "The above code doesn't work! Use ```array contains``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.64599609375,
      "end_time": 1573666726393.938
     }
    },
    "id": "jLASULZuI7Dw"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "orders_df.filter(array_contains(\"items.product\", \"fan\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAuJJGGRI7Dx"
   },
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with explode.\n",
    "\n",
    "Explode will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1255.80712890625,
      "end_time": 1573666787807.612
     }
    },
    "id": "JzGEFcZoI7Dx"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0zlLnp4I7Dx"
   },
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 746.837158203125,
      "end_time": 1573667003917.751
     }
    },
    "id": "cdGtZJR4I7Dx"
   },
   "outputs": [],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\")\n",
    "exploded_df.filter(exploded_df[\"product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3MFtKmEI7Dx"
   },
   "source": [
    "You might have tried to access the i.product column directly using a ```.filter``` right after the ```.select```. That, however, does not work, because the column is not available to ```orders_df``` when creating a clause like ```(orders_df[\"i.product\"] == \"fan\")```. A possible workaround when using Dataframe operations is that of using a string clause in ```.filter```, so that the product column will be resolved after it has been added with the ```.select```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.906005859375,
      "end_time": 1573667777707.59
     }
    },
    "id": "RXzDgj5AI7Dy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").filter(\"product = 'fan'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smURKnCRI7Dy"
   },
   "source": [
    "Project the nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 269.365966796875,
      "end_time": 1573669285846.051
     }
    },
    "id": "KwxIIJbNI7Dy"
   },
   "outputs": [],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"order_id\", \"customer.*\", \"date\", \"i.*\").limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxdSOAELI7Dy"
   },
   "source": [
    "### 1.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbaeoySMI7Dy"
   },
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. (Hint: you may need to import the function ```desc``` from ```pyspark.sql.functions``` to define descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 4287.89892578125,
      "end_time": 1573675535490.617
     }
    },
    "id": "y2-nJF5pI7Dy"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy3TVhpII7Dy"
   },
   "source": [
    "2) Find the most expensive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2290.953125,
      "end_time": 1573669705281.358
     }
    },
    "id": "LrY_mSnoI7Dz"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afpPBhRBrLuR"
   },
   "source": [
    "## 2. Spark SQL\n",
    "\n",
    "Spark SQL allows the users to formulate their queries using SQL. The requirement is the use of Dataframes, which as said before are similar to relational tables. In addition to a familiar interface, writing queries in SQL might provide better performance than RDDs, inheriting efficiency from the Dataframe operations, while also performing automatic optimization of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px2Upx-XrLuR"
   },
   "source": [
    "In order to use sql we need to create a temporary table.\n",
    "\n",
    "This table only exists for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1077.370849609375,
      "end_time": 1620481459223.37
     }
    },
    "collapsed": true,
    "id": "6ER4rWAErLuR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_df.registerTempTable(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjLuAbBDrLuR"
   },
   "source": [
    "### 2.1 Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRtQlLcorLuS"
   },
   "source": [
    "Finally, run SQL queries on the registered tables. We will run the same queries as during the previous section, but with SQL.\n",
    "\n",
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11478.6259765625,
      "end_time": 1573665795839.541
     }
    },
    "collapsed": true,
    "id": "08YF-SzQrLuS"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Finally, run SQL queries on the registered tables\n",
    "-- As you can see we can navigate to the nested items with the dot\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE orders.customer.last_name == \"Landry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLi6yF3NrLuS"
   },
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2276.55419921875,
      "end_time": 1573666251672.414
     }
    },
    "collapsed": true,
    "id": "ueKrbWtprLuT"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- How about nested arrays?\n",
    "SELECT order_id, items\n",
    "FROM orders AS o\n",
    "ORDER BY order_id\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3enkDNcNrLuT"
   },
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.202880859375,
      "end_time": 1573666528302.263
     }
    },
    "collapsed": true,
    "id": "4J1iegPzrLuU"
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE items.product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tksvS7EMrLuU"
   },
   "source": [
    "The above code doesn't work! Use ```array contains``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 752.942138671875,
      "end_time": 1573666530734.473
     }
    },
    "collapsed": true,
    "id": "2ciacrCvrLuU"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE array_contains(items.product, \"fan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCidK2FCrLuV"
   },
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with explode.\n",
    "\n",
    "Explode will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 772.9169921875,
      "end_time": 1573667016192.464
     }
    },
    "collapsed": true,
    "id": "hzQNPK2FrLuV"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT explode(items) as i, i.product, order_id\n",
    "FROM orders\n",
    "ORDER BY order_id\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pk8iNQLDrLuV"
   },
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3281.930908203125,
      "end_time": 1573667022422.047
     }
    },
    "collapsed": true,
    "id": "Lyv99R83rLuW"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Filter on product\n",
    "SELECT count(*)\n",
    "    FROM (\n",
    "    SELECT explode(items) as i, i.product, order_id\n",
    "    FROM orders\n",
    "    ORDER BY order_id\n",
    "    )\n",
    "WHERE product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuaBXyXGrLuW"
   },
   "source": [
    "You might have tried to access the i.product column directly in the same ```SELECT``` clause. That, however, does not work, because the column is not available to the ```WHERE``` clause. In order to access the built columns directly, we need to unnest the data and make it part of our ```FROM``` clause. ```LATERAL VIEW``` lets us do just that, matching each non-array attribute to an unnested row from the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 770.024169921875,
      "end_time": 1573667932258.994
     }
    },
    "collapsed": true,
    "id": "VijVTv2xrLuX"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM orders lateral view explode(items) as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsCKrVQ1rLuX"
   },
   "source": [
    "Project the nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2275.98095703125,
      "end_time": 1573667943996.512
     }
    },
    "collapsed": true,
    "id": "S39WmNLBrLuX"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT order_id, customer.first_name, customer.last_name, date, flat_items.*\n",
    "FROM orders lateral view explode(items) item_table as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_a0SvIrrLuX"
   },
   "source": [
    "Having built an unnested table, we can now easily aggregate over the previously nested columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6Ude1QurLuY"
   },
   "source": [
    "### 2.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW3iP8J3rLuY"
   },
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2274.546142578125,
      "end_time": 1573669714658.905
     }
    },
    "collapsed": true,
    "id": "gjpEnAhErLuY"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "# Your SQL Query Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0lmVfTfrLuY"
   },
   "source": [
    "2) Find the most expensive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1268.054931640625,
      "end_time": 1573669716818.317
     }
    },
    "collapsed": true,
    "id": "UDofEe7MrLuY"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "# Your SQL Query Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qPak0E_rLuZ"
   },
   "source": [
    "## 3. Create Nestedness\n",
    "\n",
    "We've already had a look at the solution of dataframes/SparkSQL towards unnesting arrays by using <font face=\"courier\">explode</font> method. For the other way round, Spark Dataframes / Spark SQL also provide ways for us to nest our data by creating arrays, especially after clauses like <font face=\"courier\">group by</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCUYQYzawGkZ"
   },
   "source": [
    "In traditional PostgreSQL, we have to use one of the aggregation functions (<font face=\"courier\">max, sum, count,</font> ...) to process the result after the <font face=\"courier\">group by</font> operation. For example, for each customer (assume there are no customers with both the same first name and last name), we want to find out how many dates at which they placed an order. The query should look like this (fill in the queries yourself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11936.587890625,
      "end_time": 1620477905132.344
     }
    },
    "id": "5FI98iZ70Ipm"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 14957.180908203125,
      "end_time": 1620477984175.839
     }
    },
    "colab": {
     "referenced_widgets": [
      "fd7fe3bdf4c64325a8320ff980c20670",
      "a9efa0cac1714601a48834f0af4eb3fb"
     ]
    },
    "id": "FunDHOxO1zPs",
    "outputId": "8835267b-ab38-4bc5-ce5f-32ca505001bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7fe3bdf4c64325a8320ff980c20670"
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9efa0cac1714601a48834f0af4eb3fb"
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "# Your SQL Query Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsTLU8ml2ORG"
   },
   "source": [
    "But what if we are interested not only in the number of dates, but the actual\n",
    "dates themselves? Luckily Spark Dataframes / Spark SQL do provide us with methods to preserve the original information of the date list. If now we would like to know for each customer, on which dates they placed an order, we shall use <font face=\"courier\">collect_set</font> method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1418.15087890625,
      "end_time": 1620478080466.68
     }
    },
    "id": "h1oGXmWt3gss"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(collect_set(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 10117.97900390625,
      "end_time": 1620478640139.884
     }
    },
    "id": "aJW1JpzQ3lEV"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select customer.first_name, customer.last_name, collect_set(\"date\") from orders group by customer.first_name, customer.last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2rTR-O78gy3"
   },
   "source": [
    "For some other cases where we want to preserve all the entries rather than the de-duplicated ones, we can use  <font face=\"courier\">collect_list</font> method. For example, for each date we want to record the first and last names of customers. Since two customers might share the same last name, we need to collect all of them. The query should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 12271.528076171875,
      "end_time": 1620480910312.875
     }
    },
    "id": "X_PB-uAH9I5M"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "orders_df.groupBy(\"date\").agg(collect_list(\"customer.last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30449.055908203125,
      "end_time": 1620481513099.958
     }
    },
    "colab": {
     "referenced_widgets": [
      "a2f4e8e80a2e424ba0e552e89b16a8a7",
      "2b9d668dbcfa4a639a140d117b5e3902"
     ]
    },
    "id": "z7DSOsKC9dmF",
    "outputId": "eae40ebd-63f9-4ec1-9742-f1bb391d1071"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f4e8e80a2e424ba0e552e89b16a8a7"
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9d668dbcfa4a639a140d117b5e3902"
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select \"date\", collect_list(customer.last_name) from orders group by \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoJbUmwrQOmy"
   },
   "source": [
    "Now try it on yourself.\n",
    "\n",
    "For every order in 2016-1-1, return the list of products that appeared in that order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 90.6279296875,
      "end_time": 1620480224562.293
     }
    },
    "id": "GpWf6W_ZPg9-"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ifIhGcEQmrT"
   },
   "source": [
    "For every product, return the set of dates at which it's purchased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 996.149169921875,
      "end_time": 1620480623060.309
     }
    },
    "id": "ZEmGI5GUPg9-"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uboC94LIRThc"
   },
   "source": [
    "One of the drawbacks of the <font face=\"courier\">collect_set/collect_list</font> method is they only accept one column as the argument. Later we will see how we can create nestedness on pretty much everything after we get the hang of the mighty JSONiq."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise10_DF_SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
