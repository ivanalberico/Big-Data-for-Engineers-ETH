{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "2021_Exercises_exercise03_Exercise03_HDFS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDgHQGxb_D02"
      },
      "source": [
        "# <center>Big Data for Engineers &ndash; Exercises</center>\n",
        "## <center>Spring 2021 &ndash; Week 3 &ndash; ETH Zurich</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK6kpbgI_D04"
      },
      "source": [
        "## Introduction\n",
        "This week we will cover mostly theoretical aspects of Hadoop and HDFS and we will discuss advantages and limitations of different storage models.\n",
        "\n",
        "#### What is Hadoop?\n",
        "_\"Hadoop provides a **distributed file system** and a\n",
        "**framework for the analysis and transformation** of very **large**\n",
        "data sets using the MapReduce paradigm.\"_ [1] \n",
        "\n",
        "Several components are part of this framework. In this course you will study HDFS, MapReduce and HBase while this exercise focuses on HDFS and storage models.\n",
        "\n",
        "\n",
        "| *Component*                |*Description*  |*First developer*  |\n",
        "|----------------------------------------------|---|---|\n",
        "| **HDFS**                  |Distributed file system  |Yahoo!  |\n",
        "| **MapReduce**   |Distributed computation framework   |Yahoo!  |\n",
        "| **HBase**           | Column-oriented table service  |Powerset (Microsoft)  |\n",
        "| Pig  | Dataflow language and parallel execution framework  |Yahoo!   |\n",
        "| Hive            |Data warehouse infrastructure   |Facebook  |\n",
        "| ZooKeeper    |Distributed coordination service   |Yahoo!  |\n",
        "| Chukwa  |System for collecting management data   |Yahoo!  |\n",
        "| Avro                |Data serialization system   |Yahoo! + Cloudera  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fTkcORr_D05"
      },
      "source": [
        "## 1. The Hadoop Distributed File System\n",
        "### 1.1 &ndash; State which of the following statements are true:\n",
        "\n",
        "1. The HDFS namespace is a hierarchy of files and directories.\n",
        "\n",
        "1. In HDFS, each block of the file is either 64 or 128 megabytes depending on the version and distribution of Hadoop in use, and this cannot be changed.\n",
        "\n",
        "1. A client wanting to write a file into HDFS, first contacts the NameNode, then sends the data to it. The NameNode will write the data into multiple DataNodes in a pipelined fashion. \n",
        "\n",
        "1. A DataNode may execute multiple application tasks for different clients concurrently.\n",
        "\n",
        "1. The cluster can have thousands of DataNodes and tens of thousands of HDFS clients per cluster.\n",
        "\n",
        "1. HDFS NameNodes keep the namespace in RAM.\n",
        "\n",
        "1. The locations of block replicas are part of the persistent checkpoint that the NameNode stores in its native file system.\n",
        "\n",
        "1. If the block size is set to 64 megabytes, storing a file of 80 megabytes will actually require 128 megabytes of physical memory (2 blocks of 64 megabytes each). \n",
        "\n",
        "\n",
        "### 1.2 &ndash; A typical filesystem block size is 4096 bytes. How large is a block in HDFS? List at least two advantages of such choice.\n",
        "\n",
        "\n",
        "### 1.3 &ndash; How does the hardware cost grow as function of the amount of data we need to store in a Distributed File System such as HDFS? Why?\n",
        "\n",
        "\n",
        "### 1.4 &ndash; Scalability, Durability and Performance on HDFS\n",
        "Explain how HDFS accomplishes the following requirements:\n",
        "\n",
        "1. Scalability\n",
        "\n",
        "1. Durability\n",
        "\n",
        "1. High sequential read/write performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlmEHwbO_D06"
      },
      "source": [
        "## 2. File I/O operations and replica management.\n",
        "\n",
        "\n",
        "### 2.1 &ndash; Replication policy\n",
        "Assume your HDFS cluster is made of 3 racks, each containing 3 DataNodes. Assume also the HDFS is configured to use a block size of 100 megabytes and that a client is connecting from outside the datacenter (therefore no DataNode is priviledged). \n",
        "\n",
        "1. The client uploads a file of 150 megabytes. Draw in the picture below a possible blocks configuration according to the default HDFS replica policy. How many replicas are there for each block? Where are these replicas stored?\n",
        "\n",
        "1. Can you find a with a different policy that, using the same number of replicas, improves the expected availability of a block? Does your solution show any drawbacks?\n",
        "\n",
        "1. Referring to the picture below, assume a block is stored in Node 3, as well as in Node 4 and Node 5. If this block of data has to be processed by a task running on Node 6, which of the three replicas will be actually read by Node 6? \n",
        "\n",
        "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex03/cluster.jpg\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxcZcVegBW3Y"
      },
      "source": [
        "### 2.2 &ndash; File read and write data flow.\r\n",
        "To get an idea of how data flows between the client interacting with HDFS, consider a diagram below which shows main components of HDFS. \r\n",
        "\r\n",
        "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex03/clientHDFS.jpg\" width=\"600\">\r\n",
        "\r\n",
        "1. Draw the main sequence of events when a client copies a file to HDFS.\r\n",
        "2. Draw the main sequence of events when a client reads a file from HDFS.\r\n",
        "3. Why do you think a client writes data directly to datanodes instead of sending it through the namenode?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGMvdeh_D08"
      },
      "source": [
        "## 3. Storage models\n",
        "\n",
        "### 3.1 &ndash; List two differences between Object Storage and Block Storage.\n",
        "\n",
        "### 3.2 &ndash; Compare Object Storage and Block Storage. For each of the following use cases, say which technology (object or block storage) better fits the requirements and briefly justify why.\n",
        "\n",
        "1. Store Netflix movie files in such a way they are accessible from many client applications at the same time _[Object storage | Block Storage ]_\n",
        "\n",
        "1. Store experimental and simulation data from CERN _[Object storage | Block Storage ]_\n",
        "\n",
        "1. Store the auto-backups of iPhone/Android devices _[Object storage | Block Storage ]_\n",
        "\n",
        "\n",
        "\n",
        "### 3.3 &ndash; Cost of Object Storage (Optional)\n",
        "Azure Object Storage offers different access tiers, which allow you to store blob object data in the most cost-effective manner. \n",
        "\n",
        "Imagine you want to have a copy of your important documents, photos and videos to both ensure durability and to share them accross your several devices. You need about 600GB of storage space. Two possibilities would be [hot and cool storage tiers on Azure Blob Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers). \n",
        "\n",
        "1. Explain the diffrerence between hot and cool storages. \n",
        "\n",
        "1. Compare costs of cool and hot storages on [Azure Storage](https://azure.microsoft.com/en-us/pricing/details/storage/blobs/). Explain why prices are different. Which one would you choose? Why?\n",
        "\n",
        "1. Another possibile solution would be to pay for a [Dropbox Pro account](https://www.dropbox.com/business/plans-comparison). Compare costs of Dropbox and Azure storage. Which one is cheaper?\n",
        "\n",
        "1. What about buying an external hard drive? List two advantages and two disadvantages of this solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmVgug9O_D09"
      },
      "source": [
        "## Explore on your own (Optional)\n",
        "\n",
        "*Note: The best way to have to understand how HDFS works under the hood is to install it and play with it. The following exercise guidse you through the process of installing Hadoop on your local computer. This, however, might be tricky and it is only for the most adventurous. During the exercise session, we will show the main takeaways.*\n",
        "\n",
        "## 4. Install Hadoop\n",
        "\n",
        "You can now install Hadoop on your local computer to test HDFS in practice, we are going to try the last version  `3.3.0`. Notice that installing Hadoop on a single machine is not very useful for a realm application, but neverthless this allows you to better understand how it works under the hood.\n",
        "\n",
        "The installation on GNU/Linux is rather easy, [follow this steps](https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/SingleCluster.html). With a similar procedure you can also install on [Windows](https://kontext.tech/column/hadoop/447/install-hadoop-330-on-windows-10-step-by-step-guide) \\([alternative old guide](http://wiki.apache.org/hadoop/Hadoop2OnWindows)) or [Mac OSX](https://towardsdatascience.com/installing-hadoop-on-a-mac-ec01c67b003c) \\([alternative old guide](http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html)).\n",
        "\n",
        "During next recitation sessions, we will use a cluster version of HDFS on Azure. The purpose of this session is to familiarise you with shell commands and web Interface by using your local machines. \n",
        "\n",
        "Each Hadoop cluster is set up in one of the three supported modes:\n",
        "\n",
        "- Local (Standalone) Mode\n",
        "- Pseudo-Distributed Mode\n",
        "- Fully-Distributed Mode\n",
        "\n",
        "By default Hadoop runs in Local Mode but you should set-up it for **Pseudo-Distributed Mode**. This will allow you to run Hadoop on a single-node (your computer) simulating a distributed file system. As explained in the tutorials, to set up a Pseudo-Distributed Mode you will need to edit `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` as follows:\n",
        "\n",
        "* `etc/hadoop/core-site.xml`:\n",
        "\n",
        "```xml\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "* `etc/hadoop/hdfs-site.xml`:\n",
        "\n",
        "```xml\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "Hadoop distribution requires having preinstalled Java and knowing the root directory of it (Java 8 or 11 is required, more info [here](https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions)). \n",
        "A possible root directory of your already installed Java installation on Linux is \n",
        "\n",
        "```\n",
        " /usr/lib/jvm/default-java\n",
        "```\n",
        "\n",
        "Try making such path available through the `JAVA_HOME` environment variable. If Hadoop still can not pick it up, add it in the `etc/hadoop/hadoop-env.sh` file.\n",
        "\n",
        "It might be that by using `pdsh` you are required to set its authentication type to `ssh` by using `export PDSH_RCMD_TYPE=ssh` (more info [here](https://stackoverflow.com/questions/42756555/permission-denied-error-while-running-start-dfs-sh))\n",
        "\n",
        "Once you have formatted your filesystem (`$ bin/hdfs namenode -format`) and started the NameNode daemon (`$ sbin/start-dfs.sh`) you should be able to browse `http://localhost:9870/` and visualize the web interface of the daemon which should look similar to the following:\n",
        "\n",
        "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex03/hadoop.png\" width=\"1100\">\n",
        "\n",
        "In the `Datanodes` tab you should see a single operating datanode.\n",
        "\n",
        "### 4.1 &ndash; Upload a file into HDFS\n",
        "\n",
        "Pick an image file in your computer (or you can also download a random one) and try to upload it to HDFS. You may need to create an empty directory before uploading. (check [here](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) for help)\n",
        "\n",
        "1. Which command do you use to upload from the local file system to HDFS?\n",
        "\n",
        "1. Which information can you find if you use `Utilities -> Browse the file system` in the daemon web interface?\n",
        "\n",
        "\n",
        "### 4.2 &ndash; Local File System\n",
        "\n",
        "Now take a look at the documented default values of `hdfs-default.xml` and `core-default.xml` and locate the file that you have just uploaded into HDFS.\n",
        "\n",
        "- http://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n",
        "- http://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/core-default.xml\n",
        "\n",
        "\n",
        "1. Is the image still accessible with a normal image viewer? \n",
        "\n",
        "1. What is the path of the image relative to the HDFS filesystem?\n",
        "\n",
        "1. What is the path of the image relative to your OS filesystem?\n",
        "\n",
        "\n",
        "### 4.3 &ndash; Local File System\n",
        "\n",
        "Now try to upload a file that is ~150MB. On Unix-based system you can also generate such a file filled with zero using:\n",
        "\n",
        "```\n",
        "$ dd if=/dev/zero of=zeros.dat bs=1M count=150\n",
        "```\n",
        "\n",
        "1. How many blocks the file is split into?\n",
        "\n",
        "\n",
        "### 4.4 Stop HDFS\n",
        "\n",
        "Once you are done experimenting you can stop HDFS by running `$ sbin/stop-dfs.sh`"
      ]
    }
  ]
}